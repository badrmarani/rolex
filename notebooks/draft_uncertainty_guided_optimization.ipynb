{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tinp_size: int,\n",
    "\t\temb_size: int,\n",
    "\t\tlat_size: int,\n",
    "\t):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\n",
    "\t\thid_size = emb_size//2\n",
    "\t\tencode = [\n",
    "\t\t\tnn.Linear(inp_size, emb_size), nn.Tanh(), nn.Dropout(0.5),\n",
    "\t\t\tnn.Linear(emb_size, hid_size), nn.Tanh(), nn.Dropout(0.5),\n",
    "\t\t]\n",
    "\n",
    "\t\tself.encode = nn.Sequential(*encode)\n",
    "\n",
    "\t\tself.mu = nn.Linear(hid_size, lat_size)\n",
    "\t\tself.logvar = nn.Linear(hid_size, lat_size)\n",
    "\n",
    "\tdef forward(self, tensor):\n",
    "\t\ttmp = self.encode(tensor)\n",
    "\t\treturn (\n",
    "\t\t\tself.mu(tmp),\n",
    "\t\t\tself.logvar(tmp),\n",
    "\t\t)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tlat_size: int,\n",
    "\t\temb_size: int,\n",
    "\t\tout_size: int,\n",
    "\t):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\n",
    "\t\thid_size = emb_size//2\n",
    "\t\tdecode = [\n",
    "\t\t\tnn.Linear(lat_size, hid_size), nn.Tanh(), nn.Dropout(0.5),\n",
    "\t\t\tnn.Linear(hid_size, emb_size), nn.Tanh(), nn.Dropout(0.5),\n",
    "\t\t\tnn.Linear(emb_size, out_size),\n",
    "                    nn.Sigmoid(),\n",
    "\t\t]\n",
    "\n",
    "\t\tself.decode = nn.Sequential(*decode)\n",
    "\n",
    "\tdef forward(self, tensor):\n",
    "\t\treturn self.decode(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDE(log_a, log_b):\n",
    "\n",
    "    max_log = torch.max(log_a, log_b)\n",
    "    min_log = torch.min(log_a, log_b)\n",
    "    return max_log + torch.log(1 + torch.exp(min_log - max_log))\n",
    "\n",
    "\n",
    "def log_gaussian_likelihood(x_recon, x):\n",
    "    std = torch.ones(x_recon)\n",
    "    # return torch.distributions.Normal(\n",
    "    #     x_recon, std,\n",
    "    # ).log_prob(x).sum(-1)\n",
    "\n",
    "    # is this more stable?\n",
    "    return x_recon + std*torch.randn_like(std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, inp_size, emb_size, lat_size):\n",
    "\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encode = Encoder(inp_size, emb_size, lat_size).to(torch.float)\n",
    "        self.decode = Decoder(lat_size, emb_size, inp_size).to(torch.float)\n",
    "\n",
    "        self.lat_size = lat_size\n",
    "\n",
    "        self.uncertainty_threshold_value = 2000.0\n",
    "        self.n_gradient_steps = 10\n",
    "        self.gradient_scale = 1e-3\n",
    "        self.n_simulations = 100\n",
    "        self.n_sampled_outcomes = 100\n",
    "\n",
    "    def reparameterization(self, mu, logvar):\n",
    "        eps = torch.randn_like(logvar)\n",
    "        std = logvar.mul(0.5).exp()\n",
    "        return mu + std * eps\n",
    "\n",
    "    def loss(self, x, x_recon, mu, logvar):\n",
    "        bs = x.size(0)\n",
    "        rec_loss = nn.functional.mse_loss(\n",
    "            x_recon.view(bs, -1),\n",
    "            x.view(bs, -1),\n",
    "            reduction=\"none\",\n",
    "        ).sum(dim=-1)\n",
    "\n",
    "        kld_loss = -0.5 * torch.sum(1+logvar-mu.pow(2)-logvar.exp(), dim=-1)\n",
    "\n",
    "        return (\n",
    "            (rec_loss + kld_loss).mean(dim=0),\n",
    "            rec_loss.mean(dim=0),\n",
    "            kld_loss.mean(dim=0),\n",
    "        )\n",
    "\n",
    "    def enable_dropout(self):\n",
    "        for m in self.modules():\n",
    "            if m.__class__.__name__.startswith(\"Dropout\"):\n",
    "                m.train()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, n_samples, device):\n",
    "        z = torch.randn((n_samples, self.lat_size), device=device)\n",
    "        return self.decode(z)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reconstruct(self, tensor):\n",
    "        return self.forward(tensor)[0]\n",
    "\n",
    "    def auxiliary_net(self, tensor):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.lat_size, self.lat_size//2), nn.Tanh(),\n",
    "            nn.Linear(self.lat_size//2, 1),\n",
    "        )(tensor).to(torch.float)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        mu, logvar = self.encode(tensor)\n",
    "        z = self.reparameterization(mu, logvar)\n",
    "\n",
    "        z = self.gradient_ascent_optimisation(\n",
    "            sample_source=tensor,\n",
    "            sample_latent_space=z,\n",
    "            auxiliary_net = self.auxiliary_net,\n",
    "            uncertainty_threshold_value = self.uncertainty_threshold_value,\n",
    "            n_gradient_steps = self.n_gradient_steps,\n",
    "            gradient_scale = self.gradient_scale,\n",
    "            n_simulations = self.n_simulations,\n",
    "            n_sampled_outcomes = self.n_sampled_outcomes,\n",
    "        )\n",
    "\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "\n",
    "    def gradient_ascent_optimisation(\n",
    "        self,\n",
    "        sample_source,\n",
    "        sample_latent_space,\n",
    "        auxiliary_net,\n",
    "        uncertainty_threshold_value,\n",
    "        n_gradient_steps,\n",
    "        gradient_scale,\n",
    "        n_simulations,\n",
    "        n_sampled_outcomes,\n",
    "    ):\n",
    "        for _ in range(n_gradient_steps):\n",
    "            # self.zero_grad()\n",
    "\n",
    "            tmp = sample_latent_space.requires_grad_()\n",
    "            p = auxiliary_net(tmp)\n",
    "\n",
    "            gradient = torch.autograd.grad(\n",
    "                outputs=p,\n",
    "                inputs=sample_latent_space,\n",
    "                grad_outputs=torch.ones_like(p),\n",
    "                retain_graph=False,\n",
    "            )[0]\n",
    "\n",
    "            gradient /= gradient.norm(2)\n",
    "\n",
    "            tmp = sample_latent_space + gradient * gradient_scale\n",
    "\n",
    "            mi = self.importance_sampling_mi(\n",
    "                sample_source=sample_source,\n",
    "                sample_latent_space=tmp,\n",
    "                n_simulations=n_simulations,\n",
    "                n_sampled_outcomes=n_sampled_outcomes,\n",
    "            )\n",
    "\n",
    "            mask = (mi <= uncertainty_threshold_value)\n",
    "            mask = mask.unsqueeze(-1).repeat(1, 2)\n",
    "            sample_latent_space = torch.where(mask, tmp, sample_latent_space)\n",
    "        \n",
    "        return sample_latent_space\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def importance_sampling_mi(\n",
    "        self,\n",
    "        sample_source: torch.Tensor,\n",
    "        sample_latent_space: torch.Tensor,\n",
    "        n_simulations: int,\n",
    "        n_sampled_outcomes: int,\n",
    "    ) -> torch.Tensor:\n",
    "        log_mi = []\n",
    "\n",
    "        self.train()\n",
    "        for s in range(n_simulations):\n",
    "            all_log_psm = []\n",
    "            \n",
    "            x_recon = self.decode(sample_latent_space)\n",
    "            \n",
    "            for m in range(n_sampled_outcomes):        \n",
    "                self.eval(); self.enable_dropout()\n",
    "                \n",
    "                log_psm = log_gaussian_likelihood(x_recon, sample_source)\n",
    "\n",
    "                all_log_psm.append(log_psm)\n",
    "\n",
    "            all_log_psm = torch.stack(all_log_psm, dim=1)\n",
    "            log_ps = - torch.log(torch.tensor(n_sampled_outcomes).float()) + torch.logsumexp(all_log_psm, dim=1)\n",
    "            \n",
    "            right_log_hs = log_ps + torch.log(-log_ps)\n",
    "            psm_log_psm = all_log_psm + torch.log(-all_log_psm)\n",
    "            left_log_hs = - torch.log(torch.tensor(n_sampled_outcomes).float()) + torch.logsumexp(psm_log_psm, dim=1)\n",
    "\n",
    "            tmp_log_hs = LDE(left_log_hs, right_log_hs) - log_ps\n",
    "            log_mi.append(tmp_log_hs)\n",
    "\n",
    "        log_mi = torch.stack(log_mi, dim=1)\n",
    "        log_mi_avg = - torch.log(torch.tensor(n_simulations).float()) + torch.logsumexp(log_mi, dim=1)\n",
    "        return log_mi_avg.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "net = VAE(\n",
    "    inp_size=20,\n",
    "    emb_size=20,\n",
    "    lat_size=2,\n",
    ").to(device)\n",
    "\n",
    "for i in range(20):\n",
    "    x = torch.randn((10, 20))\n",
    "    x_recon, mu, logvar = net(x)\n",
    "    loss, rec, kld = net.loss(x, x_recon, mu, logvar)\n",
    "\n",
    "    print(i, loss.item(), rec.item(), kld.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uq-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
