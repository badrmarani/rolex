{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tinp_size: int,\n",
    "\t\temb_size: int,\n",
    "\t\tlat_size: int,\n",
    "\t):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\n",
    "\t\thid_size = emb_size//2\n",
    "\t\tencode = [\n",
    "\t\t\tnn.Linear(inp_size, emb_size), nn.Tanh(), nn.Dropout(0.5),\n",
    "\t\t\tnn.Linear(emb_size, hid_size), nn.Tanh(), nn.Dropout(0.5),\n",
    "\t\t]\n",
    "\n",
    "\t\tself.encode = nn.Sequential(*encode)\n",
    "\n",
    "\t\tself.mu = nn.Linear(hid_size, lat_size)\n",
    "\t\tself.logvar = nn.Linear(hid_size, lat_size)\n",
    "\n",
    "\tdef forward(self, tensor):\n",
    "\t\ttmp = self.encode(tensor)\n",
    "\t\treturn (\n",
    "\t\t\tself.mu(tmp),\n",
    "\t\t\tself.logvar(tmp),\n",
    "\t\t)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tlat_size: int,\n",
    "\t\temb_size: int,\n",
    "\t\tout_size: int,\n",
    "\t):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\n",
    "\t\thid_size = emb_size//2\n",
    "\t\tdecode = [\n",
    "\t\t\tnn.Linear(lat_size, hid_size), nn.Tanh(), nn.Dropout(0.5),\n",
    "\t\t\tnn.Linear(hid_size, emb_size), nn.Tanh(), nn.Dropout(0.5),\n",
    "\t\t\tnn.Linear(emb_size, out_size),\n",
    "                    nn.Sigmoid(),\n",
    "\t\t]\n",
    "\n",
    "\t\tself.decode = nn.Sequential(*decode)\n",
    "\n",
    "\tdef forward(self, tensor):\n",
    "\t\treturn self.decode(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDE(log_a, log_b):\n",
    "\n",
    "    max_log = torch.max(log_a, log_b)\n",
    "    min_log = torch.min(log_a, log_b)\n",
    "    return max_log + torch.log(1 + torch.exp(min_log - max_log))\n",
    "\n",
    "\n",
    "def log_gaussian_likelihood(x_recon, x):\n",
    "    std = torch.ones(x_recon)\n",
    "    # return torch.distributions.Normal(\n",
    "    #     x_recon, std,\n",
    "    # ).log_prob(x).sum(-1)\n",
    "\n",
    "    # is this more stable?\n",
    "    return x_recon + std*torch.randn_like(std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, inp_size, emb_size, lat_size):\n",
    "\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encode = Encoder(inp_size, emb_size, lat_size).to(torch.float)\n",
    "        self.decode = Decoder(lat_size, emb_size, inp_size).to(torch.float)\n",
    "\n",
    "        self.lat_size = lat_size\n",
    "\n",
    "        self.uncertainty_threshold_value = 2000.0\n",
    "        self.n_gradient_steps = 10\n",
    "        self.gradient_scale = 1e-3\n",
    "        self.n_simulations = 100\n",
    "        self.n_sampled_outcomes = 100\n",
    "\n",
    "    def reparameterization(self, mu, logvar):\n",
    "        eps = torch.randn_like(logvar)\n",
    "        std = logvar.mul(0.5).exp()\n",
    "        return mu + std * eps\n",
    "\n",
    "    def loss(self, x, x_recon, mu, logvar):\n",
    "        bs = x.size(0)\n",
    "        rec_loss = nn.functional.mse_loss(\n",
    "            x_recon.view(bs, -1),\n",
    "            x.view(bs, -1),\n",
    "            reduction=\"none\",\n",
    "        ).sum(dim=-1)\n",
    "\n",
    "        kld_loss = -0.5 * torch.sum(1+logvar-mu.pow(2)-logvar.exp(), dim=-1)\n",
    "\n",
    "        return (\n",
    "            (rec_loss + kld_loss).mean(dim=0),\n",
    "            rec_loss.mean(dim=0),\n",
    "            kld_loss.mean(dim=0),\n",
    "        )\n",
    "\n",
    "    def enable_dropout(self):\n",
    "        for m in self.modules():\n",
    "            if m.__class__.__name__.startswith(\"Dropout\"):\n",
    "                m.train()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, n_samples, device):\n",
    "        z = torch.randn((n_samples, self.lat_size), device=device)\n",
    "        return self.decode(z)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reconstruct(self, tensor):\n",
    "        return self.forward(tensor)[0]\n",
    "\n",
    "    def auxiliary_net(self, tensor):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.lat_size, self.lat_size//2), nn.Tanh(),\n",
    "            nn.Linear(self.lat_size//2, 1),\n",
    "        )(tensor).to(torch.float)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        mu, logvar = self.encode(tensor)\n",
    "        z = self.reparameterization(mu, logvar)\n",
    "\n",
    "        z = self.gradient_ascent_optimisation(\n",
    "            sample_source=tensor,\n",
    "            sample_latent_space=z,\n",
    "            auxiliary_net = self.auxiliary_net,\n",
    "            uncertainty_threshold_value = self.uncertainty_threshold_value,\n",
    "            n_gradient_steps = self.n_gradient_steps,\n",
    "            gradient_scale = self.gradient_scale,\n",
    "            n_simulations = self.n_simulations,\n",
    "            n_sampled_outcomes = self.n_sampled_outcomes,\n",
    "        )\n",
    "\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "\n",
    "    def gradient_ascent_optimisation(\n",
    "        self,\n",
    "        sample_source,\n",
    "        sample_latent_space,\n",
    "        auxiliary_net,\n",
    "        uncertainty_threshold_value,\n",
    "        n_gradient_steps,\n",
    "        gradient_scale,\n",
    "        n_simulations,\n",
    "        n_sampled_outcomes,\n",
    "    ):\n",
    "        for _ in range(n_gradient_steps):\n",
    "            # self.zero_grad()\n",
    "\n",
    "            tmp = sample_latent_space.requires_grad_()\n",
    "            p = auxiliary_net(tmp)\n",
    "\n",
    "            gradient = torch.autograd.grad(\n",
    "                outputs=p,\n",
    "                inputs=sample_latent_space,\n",
    "                grad_outputs=torch.ones_like(p),\n",
    "                retain_graph=False,\n",
    "            )[0]\n",
    "\n",
    "            gradient /= gradient.norm(2)\n",
    "\n",
    "            tmp = sample_latent_space + gradient * gradient_scale\n",
    "\n",
    "            mi = self.importance_sampling_mi(\n",
    "                sample_source=sample_source,\n",
    "                sample_latent_space=tmp,\n",
    "                n_simulations=n_simulations,\n",
    "                n_sampled_outcomes=n_sampled_outcomes,\n",
    "            )\n",
    "\n",
    "            mask = (mi <= uncertainty_threshold_value)\n",
    "            mask = mask.unsqueeze(-1).repeat(1, 2)\n",
    "            sample_latent_space = torch.where(mask, tmp, sample_latent_space)\n",
    "        \n",
    "        return sample_latent_space\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def importance_sampling_mi(\n",
    "        self,\n",
    "        sample_source: torch.Tensor,\n",
    "        sample_latent_space: torch.Tensor,\n",
    "        n_simulations: int,\n",
    "        n_sampled_outcomes: int,\n",
    "    ) -> torch.Tensor:\n",
    "        log_mi = []\n",
    "\n",
    "        self.train()\n",
    "        for s in range(n_simulations):\n",
    "            all_log_psm = []\n",
    "            \n",
    "            x_recon = self.decode(sample_latent_space)\n",
    "            \n",
    "            for m in range(n_sampled_outcomes):        \n",
    "                self.eval(); self.enable_dropout()\n",
    "                \n",
    "                log_psm = log_gaussian_likelihood(x_recon, sample_source)\n",
    "\n",
    "                all_log_psm.append(log_psm)\n",
    "\n",
    "            all_log_psm = torch.stack(all_log_psm, dim=1)\n",
    "            log_ps = - torch.log(torch.tensor(n_sampled_outcomes).float()) + torch.logsumexp(all_log_psm, dim=1)\n",
    "            \n",
    "            right_log_hs = log_ps + torch.log(-log_ps)\n",
    "            psm_log_psm = all_log_psm + torch.log(-all_log_psm)\n",
    "            left_log_hs = - torch.log(torch.tensor(n_sampled_outcomes).float()) + torch.logsumexp(psm_log_psm, dim=1)\n",
    "\n",
    "            tmp_log_hs = LDE(left_log_hs, right_log_hs) - log_ps\n",
    "            log_mi.append(tmp_log_hs)\n",
    "\n",
    "        log_mi = torch.stack(log_mi, dim=1)\n",
    "        log_mi_avg = - torch.log(torch.tensor(n_simulations).float()) + torch.logsumexp(log_mi, dim=1)\n",
    "        return log_mi_avg.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 24.693035125732422 24.500940322875977 0.19209758937358856\n",
      "1 22.13822364807129 21.926074981689453 0.21214886009693146\n",
      "2 25.800228118896484 25.685291290283203 0.11493788659572601\n",
      "3 24.170717239379883 23.970470428466797 0.20024721324443817\n",
      "4 25.3535099029541 25.100893020629883 0.25261780619621277\n",
      "5 29.40237045288086 29.183673858642578 0.21870103478431702\n",
      "6 24.657785415649414 24.446958541870117 0.21082532405853271\n",
      "7 24.559932708740234 24.326129913330078 0.23380358517169952\n",
      "8 24.407089233398438 24.1723690032959 0.23471815884113312\n",
      "9 27.98561668395996 27.796710968017578 0.1889064460992813\n",
      "10 23.222103118896484 22.931554794311523 0.2905489206314087\n",
      "11 26.150650024414062 25.95595359802246 0.1946951448917389\n",
      "12 24.565067291259766 24.367507934570312 0.19755737483501434\n",
      "13 29.494882583618164 29.263010025024414 0.23187215626239777\n",
      "14 27.367679595947266 27.22023582458496 0.14744099974632263\n",
      "15 26.735210418701172 26.596588134765625 0.13862422108650208\n",
      "16 26.092947006225586 25.820053100585938 0.27289634943008423\n",
      "17 18.162715911865234 17.974445343017578 0.18827158212661743\n",
      "18 21.52457046508789 21.333003997802734 0.19156485795974731\n",
      "19 20.502222061157227 20.328296661376953 0.17392496764659882\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "net = VAE(\n",
    "    inp_size=20,\n",
    "    emb_size=20,\n",
    "    lat_size=2,\n",
    ").to(device)\n",
    "\n",
    "for i in range(20):\n",
    "    x = torch.randn((10, 20))\n",
    "    x_recon, mu, logvar = net(x)\n",
    "    loss, rec, kld = net.loss(x, x_recon, mu, logvar)\n",
    "\n",
    "    print(i, loss.item(), rec.item(), kld.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uq-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
